{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Artigo Mestrado",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQCnYPVDrsgx",
        "colab_type": "text"
      },
      "source": [
        "# PROCESSO DE TREINAMENTO DA REDE PARA IMAGENS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhzxsJb3dpWq",
        "colab_type": "text"
      },
      "source": [
        "## Configurações iniciais\n",
        "\n",
        "Aqui podemos encontrar mais modelos:[Tensorflow detection model zoo: COCO-trained models](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models), e também seus respectivos arquivos de configuração [object_detection/samples/configs/](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnNXNQCjdniL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "repo_url = 'https://github.com/AlvaroCavalcante/tf-models'\n",
        "\n",
        "num_steps = 600\n",
        "\n",
        "num_eval_steps = 50\n",
        "\n",
        "MODELS_CONFIG = {\n",
        "    'ssd_mobilenet_v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n",
        "        'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    'faster_rcnn_inception_v2': {\n",
        "        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n",
        "        'pipeline_file': 'faster_rcnn_inception_v2_pets.config',\n",
        "        'batch_size': 4\n",
        "    },\n",
        "    'ssd_inception_v2': {\n",
        "        'model_name': 'ssd_inception_v2_coco_2018_01_28',\n",
        "        'pipeline_file': 'ssd_inception_v2_coco.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    'ssd_resnet_50': {\n",
        "        'model_name': 'ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03',\n",
        "        'pipeline_file': 'ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config',\n",
        "        'batch_size': 8\n",
        "    }\n",
        "    \n",
        "}\n",
        "\n",
        "selected_model = 'ssd_resnet_50'\n",
        "\n",
        "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
        "\n",
        "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n",
        "\n",
        "batch_size = MODELS_CONFIG[selected_model]['batch_size']\n",
        "\n",
        "use_coco_checkpoint = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4V-XE6kbkc1",
        "colab_type": "text"
      },
      "source": [
        "### Clonando o repositório usado para o treinamento!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxc3DmvLQF3z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "6f853e0a-944f-48ef-c00b-93c073b49548"
      },
      "source": [
        "import os\n",
        "\n",
        "%cd /content\n",
        "\n",
        "repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n",
        "\n",
        "!git clone {repo_url}\n",
        "%cd {repo_dir_path}\n",
        "!git pull"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'tf-models'...\n",
            "remote: Enumerating objects: 39493, done.\u001b[K\n",
            "remote: Total 39493 (delta 0), reused 0 (delta 0), pack-reused 39493\u001b[K\n",
            "Receiving objects: 100% (39493/39493), 570.73 MiB | 25.21 MiB/s, done.\n",
            "Resolving deltas: 100% (25756/25756), done.\n",
            "/content/tf-models\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI8__uNS8-ns",
        "colab_type": "text"
      },
      "source": [
        "## Instalando os pacotes necessários"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecpHEnka8Kix",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "70105068-e019-4d3e-ea92-7ebc8b835672"
      },
      "source": [
        "%cd /content\n",
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "\n",
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib\n",
        "\n",
        "!pip install -q pycocotools\n",
        "\n",
        "!pip install gast==0.2.2\n",
        "\n",
        "!pip install tf_slim\n",
        "\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n",
        "\n",
        "!python object_detection/builders/model_builder_test.py\n",
        "!pip install tensorflow-object-detection-api"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Selecting previously unselected package python-bs4.\n",
            "(Reading database ... 144379 files and directories currently installed.)\n",
            "Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n",
            "Unpacking python-bs4 (4.6.0-1) ...\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python-chardet.\n",
            "Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n",
            "Unpacking python-chardet (3.0.4-1) ...\n",
            "Selecting previously unselected package python-six.\n",
            "Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n",
            "Unpacking python-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python-webencodings.\n",
            "Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n",
            "Unpacking python-webencodings (0.5-2) ...\n",
            "Selecting previously unselected package python-html5lib.\n",
            "Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n",
            "Unpacking python-html5lib (0.999999999-1) ...\n",
            "Selecting previously unselected package python-lxml:amd64.\n",
            "Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python-olefile.\n",
            "Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n",
            "Unpacking python-olefile (0.45.1-1) ...\n",
            "Selecting previously unselected package python-pil:amd64.\n",
            "Preparing to unpack .../8-python-pil_5.1.0-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking python-pil:amd64 (5.1.0-1ubuntu0.2) ...\n",
            "Setting up python-pkg-resources (39.0.1-2) ...\n",
            "Setting up python-six (1.11.0-2) ...\n",
            "Setting up python-bs4 (4.6.0-1) ...\n",
            "Setting up python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Setting up python-olefile (0.45.1-1) ...\n",
            "Setting up python-pil:amd64 (5.1.0-1ubuntu0.2) ...\n",
            "Setting up python-webencodings (0.5-2) ...\n",
            "Setting up python-chardet (3.0.4-1) ...\n",
            "Setting up python-html5lib (0.999999999-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=b259b3dad794e6bb6766ea57420236d36570d9a1ad64aa9ba68727456d907334\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.2.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "Successfully installed gast-0.2.2\n",
            "Collecting tf_slim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf_slim) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.2.2->tf_slim) (1.12.0)\n",
            "Installing collected packages: tf-slim\n",
            "Successfully installed tf-slim-1.1.0\n",
            "/content/models/research\n",
            "object_detection/protos/input_reader.proto: warning: Import object_detection/protos/image_resizer.proto but not used.\n",
            "2020-07-07 10:33:33.282783: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Collecting tensorflow-object-detection-api\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/11/7f6d3c5c4b603cc40b2813059779afb641bd5eb68045c62ca520bfce0359/tensorflow_object_detection_api-0.1.1.tar.gz (577kB)\n",
            "\u001b[K     |████████████████████████████████| 583kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (7.0.0)\n",
            "Requirement already satisfied: Matplotlib>=2.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (3.2.2)\n",
            "Requirement already satisfied: Cython>=0.28.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (0.29.20)\n",
            "Requirement already satisfied: Protobuf in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (3.10.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (4.2.6)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (1.0.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (2.2.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (0.5.5)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (0.34.2)\n",
            "Collecting twine\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/db/b2c65078b783c6694bdfa0911bbbe0e2be7fcbc98ff23a99b8be544906b6/twine-3.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from Matplotlib>=2.1->tensorflow-object-detection-api) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from Protobuf->tensorflow-object-detection-api) (47.3.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from Protobuf->tensorflow-object-detection-api) (1.12.0)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->tensorflow-object-detection-api) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->tensorflow-object-detection-api) (5.2.2)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->tensorflow-object-detection-api) (7.5.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->tensorflow-object-detection-api) (5.6.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->tensorflow-object-detection-api) (4.7.5)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->tensorflow-object-detection-api) (4.10.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (3.2.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (1.1.2)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (2.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (1.30.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (1.12.1)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (2.2.2)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (1.4.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (0.9.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (2.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->tensorflow-object-detection-api) (1.1.0)\n",
            "Collecting readme-renderer>=21.0\n",
            "  Downloading https://files.pythonhosted.org/packages/54/e4/ed43056d80a4fcc3667e543a59cc6beaf0a3c0eade837e5591e82ad3c25a/readme_renderer-26.0-py2.py3-none-any.whl\n",
            "Collecting keyring>=15.1\n",
            "  Downloading https://files.pythonhosted.org/packages/a8/5e/d13b9feb235d042321a239ac8bc85e90cf3bbe49090c6f1383ac3fd53e0e/keyring-21.2.1-py3-none-any.whl\n",
            "Collecting pkginfo>=1.4.2\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/d5/451b913307b478c49eb29084916639dc53a88489b993530fed0a66bab8b9/pkginfo-1.5.0.1-py2.py3-none-any.whl\n",
            "Collecting requests-toolbelt!=0.9.0,>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/ef/7681134338fc097acef8d9b2f8abe0458e4d87559c689a8c306d0957ece5/requests_toolbelt-0.9.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.6/dist-packages (from twine->tensorflow-object-detection-api) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.14 in /usr/local/lib/python3.6/dist-packages (from twine->tensorflow-object-detection-api) (4.41.1)\n",
            "Collecting rfc3986>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/78/be/7b8b99fd74ff5684225f50dd0e865393d2265656ef3b4ba9eaaaffe622b8/rfc3986-1.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from twine->tensorflow-object-detection-api) (1.6.1)\n",
            "Collecting colorama>=0.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->tensorflow-object-detection-api) (5.3.4)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->tensorflow-object-detection-api) (1.0.18)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->tensorflow-object-detection-api) (5.5.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->tensorflow-object-detection-api) (2.1.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (2.11.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (5.0.7)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (0.2.0)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (4.5.3)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (0.8.3)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (4.6.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->tensorflow-object-detection-api) (3.5.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (1.4.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.4.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (3.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.8.4)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->tensorflow-object-detection-api) (19.0.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->tensorflow-object-detection-api) (1.9.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->tensorflow-object-detection-api) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->tensorflow-object-detection-api) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->tensorflow-object-detection-api) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->tensorflow-object-detection-api) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->tensorflow-object-detection-api) (1.6.0.post3)\n",
            "Requirement already satisfied: docutils>=0.13.1 in /usr/local/lib/python3.6/dist-packages (from readme-renderer>=21.0->twine->tensorflow-object-detection-api) (0.15.2)\n",
            "Collecting SecretStorage>=3; sys_platform == \"linux\"\n",
            "  Downloading https://files.pythonhosted.org/packages/c3/50/8a02cad020e949e6d7105f5f4530d41e3febcaa5b73f8f2148aacb3aeba5/SecretStorage-3.1.2-py3-none-any.whl\n",
            "Collecting jeepney>=0.4.2; sys_platform == \"linux\"\n",
            "  Downloading https://files.pythonhosted.org/packages/79/31/2e8d42727595faf224c6dbb748c32b192e212f25495fe841fb7ce8e168b8/jeepney-0.4.3-py3-none-any.whl\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->twine->tensorflow-object-detection-api) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->twine->tensorflow-object-detection-api) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->twine->tensorflow-object-detection-api) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->twine->tensorflow-object-detection-api) (2020.6.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->twine->tensorflow-object-detection-api) (3.1.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->tensorflow-object-detection-api) (0.2.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->tensorflow-object-detection-api) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->tensorflow-object-detection-api) (4.4.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->tensorflow-object-detection-api) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->tensorflow-object-detection-api) (0.7.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->jupyter->tensorflow-object-detection-api) (1.1.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->jupyter->tensorflow-object-detection-api) (2.6.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->tensorflow-object-detection-api) (0.6.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->tensorflow-object-detection-api) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->tensorflow-object-detection-api) (20.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->tensorflow-object-detection-api) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->tensorflow-object-detection-api) (4.1.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->tensorflow-object-detection-api) (4.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->tensorflow-object-detection-api) (1.3.0)\n",
            "Collecting cryptography\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/04/686efee2dcdd25aecf357992e7d9362f443eb182ecd623f882bc9f7a6bba/cryptography-2.9.2-cp35-abi3-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 31.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->tensorflow-object-detection-api) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->tensorflow-object-detection-api) (3.1.0)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography->SecretStorage>=3; sys_platform == \"linux\"->keyring>=15.1->twine->tensorflow-object-detection-api) (1.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography->SecretStorage>=3; sys_platform == \"linux\"->keyring>=15.1->twine->tensorflow-object-detection-api) (2.20)\n",
            "Building wheels for collected packages: tensorflow-object-detection-api\n",
            "  Building wheel for tensorflow-object-detection-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorflow-object-detection-api: filename=tensorflow_object_detection_api-0.1.1-cp36-none-any.whl size=844515 sha256=0d8a70b1878426dabfa34c2d7488b241f0718b0347d29d85cfbe7eafda26b6c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/54/d0/cfca11930c4b2025d40dede77059094070a67cc3e7bd3b285f\n",
            "Successfully built tensorflow-object-detection-api\n",
            "\u001b[31mERROR: readme-renderer 26.0 has requirement Pygments>=2.5.1, but you'll have pygments 2.1.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: readme-renderer, cryptography, jeepney, SecretStorage, keyring, pkginfo, requests-toolbelt, rfc3986, colorama, twine, tensorflow-object-detection-api, gast\n",
            "  Found existing installation: gast 0.2.2\n",
            "    Uninstalling gast-0.2.2:\n",
            "      Successfully uninstalled gast-0.2.2\n",
            "Successfully installed SecretStorage-3.1.2 colorama-0.4.3 cryptography-2.9.2 gast-0.3.3 jeepney-0.4.3 keyring-21.2.1 pkginfo-1.5.0.1 readme-renderer-26.0 requests-toolbelt-0.9.1 rfc3986-1.4.0 tensorflow-object-detection-api-0.1.1 twine-3.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-k7uGThXlny",
        "colab_type": "text"
      },
      "source": [
        "## Preparandos os arquivos `tfrecord`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rwmrcayx8G5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d6bcbc26-2b8b-4923-9a5b-337fa87ed7c6"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezGDABRXXhPP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "outputId": "fe7d0848-3486-49cc-8049-4c03d62b7a0d"
      },
      "source": [
        "%cd {repo_dir_path}\n",
        "\n",
        "!python research/object_detection/xml_to_csv.py -i research/object_detection/train_images -o research/object_detection/train_labels.csv -l research/object_detection\n",
        "\n",
        "!python research/object_detection/xml_to_csv.py -i research/object_detection/test_images -o research/object_detection/test_labels.csv\n",
        "\n",
        "!python research/object_detection/generate_tfrecord.py --csv_input=research/object_detection/train_labels.csv --output_path=research/object_detection/train.record --img_path=research/object_detection/train_images --label_map research/object_detection/label_map.pbtxt\n",
        "\n",
        "!python research/object_detection/generate_tfrecord.py --csv_input=research/object_detection/test_labels.csv --output_path=research/object_detection/test.record --img_path=research/object_detection/test_images --label_map research/object_detection/label_map.pbtxt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/tf-models\n",
            "Successfully converted xml to csv.\n",
            "Generate `research/object_detection/label_map.pbtxt`\n",
            "Successfully converted xml to csv.\n",
            "WARNING:tensorflow:From research/object_detection/generate_tfrecord.py:134: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From research/object_detection/generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0707 10:33:57.445550 140587895650176 module_wrapper.py:139] From research/object_detection/generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From research/object_detection/generate_tfrecord.py:53: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0707 10:33:57.511547 140587895650176 module_wrapper.py:139] From research/object_detection/generate_tfrecord.py:53: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/tf-models/research/object_detection/train.record\n",
            "WARNING:tensorflow:From research/object_detection/generate_tfrecord.py:134: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From research/object_detection/generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "W0707 10:34:01.140880 140003010590592 module_wrapper.py:139] From research/object_detection/generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From research/object_detection/generate_tfrecord.py:53: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0707 10:34:01.158862 140003010590592 module_wrapper.py:139] From research/object_detection/generate_tfrecord.py:53: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Successfully created the TFRecords: /content/tf-models/research/object_detection/test.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgd-fzAIkZlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_record_fname = '/content/tf-models/research/object_detection/test.record'\n",
        "train_record_fname = '/content/tf-models/research/object_detection/train.record'\n",
        "label_map_pbtxt_fname = '/content/tf-models/research/object_detection/label_map/label_map.pbtxt'\n",
        "\n",
        "tf_log_path = '/content/tf-models/research/object_detection/events.out.tfevents.1576020673.0083b462c1a8'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCNYAaC7w6N8",
        "colab_type": "text"
      },
      "source": [
        "## Download do modelo base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orDCj6ihgUMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f75ff7b-2cb5-40ac-8040-123c44f95140"
      },
      "source": [
        "%cd /content/models/research\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import urllib.request\n",
        "import tarfile\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DEST_DIR = '/content/models/research/pretrained_model' #saved here\n",
        "\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "    shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsLihUIedbtk",
        "colab_type": "text"
      },
      "source": [
        "## Use COCO or my own checkpoint\n",
        "If we train a new model, it's used the coco checkpoint, but if we want to continue a training, then we use your checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHnxlfRznPP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if use_coco_checkpoint:\n",
        "   fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n",
        "  \n",
        "else:\n",
        "  checkpoint_path = '/content/tf-models/research/object_detection/trained_model'\n",
        "  fine_tune_checkpoint = os.path.join(checkpoint_path, \"model.ckpt\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvwtHlLOeRJD",
        "colab_type": "text"
      },
      "source": [
        "## Configuração da pipeline de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIhw7IdpLuiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "pipeline_fname = os.path.join('/content/models/research/object_detection/samples/configs/', pipeline_file)\n",
        "\n",
        "#pipeline_path = selected_model + '_coco.config'\n",
        "\n",
        "#pipeline_fname = os.path.join('/content/object_detection_demo/', pipeline_path)\n",
        "\n",
        "assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG1nCNpUXcRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    print(categories)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    print(category_index)\n",
        "    return len(category_index.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjtCbLF2i0wI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "025eaf30-8dbc-45b8-c1f4-d1a97e57c2c5"
      },
      "source": [
        "import re\n",
        "\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname) \n",
        "\n",
        "# num_classes = 2\n",
        "\n",
        "with open(pipeline_fname) as f:\n",
        "    s = f.read()\n",
        "with open(pipeline_fname, 'w') as f:\n",
        "    \n",
        "    # fine_tune_checkpoint\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "    \n",
        "    # tfrecord files train and test.\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(train.record)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(val.record)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n",
        "\n",
        "    # label_map_path\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, num_steps\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "    \n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "    f.write(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'id': 1, 'name': 'dog'}, {'id': 2, 'name': 'cat'}]\n",
            "{1: {'id': 1, 'name': 'dog'}, 2: {'id': 2, 'name': 'cat'}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH0MEEanocn6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b0bf8b1-1d6d-4523-9aaa-c1057d8f69ec"
      },
      "source": [
        "!cat {pipeline_fname} "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# SSD with Resnet 50 v1 FPN feature extractor, shared box predictor and focal\n",
            "# loss (a.k.a Retinanet).\n",
            "# See Lin et al, https://arxiv.org/abs/1708.02002\n",
            "# Trained on COCO, initialized from Imagenet classification checkpoint\n",
            "\n",
            "# Achieves 35.2 mAP on COCO14 minival dataset. Doubling the number of training\n",
            "# steps to 50k gets 36.9 mAP\n",
            "\n",
            "# This config is TPU compatible\n",
            "\n",
            "model {\n",
            "  ssd {\n",
            "    inplace_batchnorm_update: true\n",
            "    freeze_batchnorm: false\n",
            "    num_classes: 2\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 10.0\n",
            "        x_scale: 10.0\n",
            "        height_scale: 5.0\n",
            "        width_scale: 5.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "        use_matmul_gather: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    encode_background_as_zeros: true\n",
            "    anchor_generator {\n",
            "      multiscale_anchor_generator {\n",
            "        min_level: 3\n",
            "        max_level: 7\n",
            "        anchor_scale: 4.0\n",
            "        aspect_ratios: [1.0, 2.0, 0.5]\n",
            "        scales_per_octave: 2\n",
            "      }\n",
            "    }\n",
            "    image_resizer {\n",
            "      fixed_shape_resizer {\n",
            "        height: 640\n",
            "        width: 640\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      weight_shared_convolutional_box_predictor {\n",
            "        depth: 256\n",
            "        class_prediction_bias_init: -4.6\n",
            "        conv_hyperparams {\n",
            "          activation: RELU_6,\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 0.0004\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            random_normal_initializer {\n",
            "              stddev: 0.01\n",
            "              mean: 0.0\n",
            "            }\n",
            "          }\n",
            "          batch_norm {\n",
            "            scale: true,\n",
            "            decay: 0.997,\n",
            "            epsilon: 0.001,\n",
            "          }\n",
            "        }\n",
            "        num_layers_before_predictor: 4\n",
            "        kernel_size: 3\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: 'ssd_resnet50_v1_fpn'\n",
            "      fpn {\n",
            "        min_level: 3\n",
            "        max_level: 7\n",
            "      }\n",
            "      min_depth: 16\n",
            "      depth_multiplier: 1.0\n",
            "      conv_hyperparams {\n",
            "        activation: RELU_6,\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 0.0004\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          truncated_normal_initializer {\n",
            "            stddev: 0.03\n",
            "            mean: 0.0\n",
            "          }\n",
            "        }\n",
            "        batch_norm {\n",
            "          scale: true,\n",
            "          decay: 0.997,\n",
            "          epsilon: 0.001,\n",
            "        }\n",
            "      }\n",
            "      override_base_feature_extractor_hyperparams: true\n",
            "    }\n",
            "    loss {\n",
            "      classification_loss {\n",
            "        weighted_sigmoid_focal {\n",
            "          alpha: 0.25\n",
            "          gamma: 2.0\n",
            "        }\n",
            "      }\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    normalize_loc_loss_by_codesize: true\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-8\n",
            "        iou_threshold: 0.6\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_config: {\n",
            "  fine_tune_checkpoint: \"/content/models/research/pretrained_model/model.ckpt\"\n",
            "  batch_size: 8\n",
            "  sync_replicas: true\n",
            "  startup_delay_steps: 0\n",
            "  replicas_to_aggregate: 8\n",
            "  num_steps: 600\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    random_crop_image {\n",
            "      min_object_covered: 0.0\n",
            "      min_aspect_ratio: 0.75\n",
            "      max_aspect_ratio: 3.0\n",
            "      min_area: 0.75\n",
            "      max_area: 1.0\n",
            "      overlap_thresh: 0.0\n",
            "    }\n",
            "  }\n",
            "  optimizer {\n",
            "    momentum_optimizer: {\n",
            "      learning_rate: {\n",
            "        cosine_decay_learning_rate {\n",
            "          learning_rate_base: .04\n",
            "          total_steps: 25000\n",
            "          warmup_learning_rate: .013333\n",
            "          warmup_steps: 2000\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "    }\n",
            "    use_moving_average: false\n",
            "  }\n",
            "  max_number_of_boxes: 100\n",
            "  unpad_groundtruth_tensors: false\n",
            "}\n",
            "\n",
            "train_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/tf-models/research/object_detection/train.record\"\n",
            "  }\n",
            "  label_map_path: \"/content/tf-models/research/object_detection/label_map/label_map.pbtxt\"\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  metrics_set: \"coco_detection_metrics\"\n",
            "  use_moving_averages: false\n",
            "  num_examples: 8000\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/tf-models/research/object_detection/test.record\"\n",
            "  }\n",
            "  label_map_path: \"/content/tf-models/research/object_detection/label_map/label_map.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_readers: 1\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f11w0uO3jFCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = 'training/'\n",
        "# Optionally remove content in output model directory to fresh start.\n",
        "!rm -rf {model_dir}\n",
        "os.makedirs(model_dir, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23TECXvNezIF",
        "colab_type": "text"
      },
      "source": [
        "## Rodar tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H2PZs-mSCmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "# !unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8o6r1o5SC5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LOG_DIR = model_dir\n",
        "# get_ipython().system_raw(\n",
        "#     'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "#     .format(LOG_DIR)\n",
        "# )\n",
        "\n",
        "# get_ipython().system_raw('./ngrok http 6006 &')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5GSGxZNh8rp",
        "colab_type": "text"
      },
      "source": [
        "### pegar o link do Tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjhPT9iPSJ6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "#     \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDddx2rPfex9",
        "colab_type": "text"
      },
      "source": [
        "## Treinamento do modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjDHjhKQofT5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c7d3fdb-86cf-42a8-b831-dfe03f2848ab"
      },
      "source": [
        "!python /content/models/research/object_detection/model_main.py \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --num_eval_steps={num_eval_steps}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n",
            "W0707 10:34:19.769408 140335987144576 model_lib.py:717] Forced number of epochs for all eval validations to be 1.\n",
            "INFO:tensorflow:Maybe overwriting train_steps: 600\n",
            "I0707 10:34:19.769654 140335987144576 config_util.py:552] Maybe overwriting train_steps: 600\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I0707 10:34:19.769764 140335987144576 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "I0707 10:34:19.769852 140335987144576 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n",
            "I0707 10:34:19.769938 140335987144576 config_util.py:552] Maybe overwriting eval_num_epochs: 1\n",
            "INFO:tensorflow:Maybe overwriting load_pretrained: True\n",
            "I0707 10:34:19.770012 140335987144576 config_util.py:552] Maybe overwriting load_pretrained: True\n",
            "INFO:tensorflow:Ignoring config override key: load_pretrained\n",
            "I0707 10:34:19.770083 140335987144576 config_util.py:562] Ignoring config override key: load_pretrained\n",
            "WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "W0707 10:34:19.770881 140335987144576 model_lib.py:733] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "INFO:tensorflow:create_estimator_and_inputs: use_tpu False, export_to_tpu False\n",
            "I0707 10:34:19.770986 140335987144576 model_lib.py:768] create_estimator_and_inputs: use_tpu False, export_to_tpu False\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa236589828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "I0707 10:34:19.771382 140335987144576 estimator.py:212] Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa236589828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7fa21c0aa488>) includes params argument, but params are not passed to Estimator.\n",
            "W0707 10:34:19.771616 140335987144576 model_fn.py:630] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7fa21c0aa488>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "I0707 10:34:19.772248 140335987144576 estimator_training.py:186] Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "I0707 10:34:19.772421 140335987144576 training.py:612] Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "I0707 10:34:19.772644 140335987144576 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0707 10:34:19.787123 140335987144576 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0707 10:34:19.816098 140335987144576 dataset_builder.py:83] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0707 10:34:19.820827 140335987144576 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:175: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0707 10:34:19.839683 140335987144576 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:175: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7fa21bff41d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "W0707 10:34:19.868993 140335987144576 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7fa21bff41d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7fa241b38510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "W0707 10:34:20.049975 140335987144576 ag_logging.py:146] Entity <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7fa241b38510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:80: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W0707 10:34:20.055266 140335987144576 deprecation.py:323] From /content/models/research/object_detection/inputs.py:80: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0707 10:34:20.062001 140335987144576 deprecation.py:323] From /content/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/preprocessor.py:199: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W0707 10:34:20.137044 140335987144576 deprecation.py:323] From /content/models/research/object_detection/core/preprocessor.py:199: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:261: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0707 10:34:20.235783 140335987144576 deprecation.py:323] From /content/models/research/object_detection/inputs.py:261: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0707 10:34:20.674835 140335987144576 estimator.py:1148] Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W0707 10:34:20.685099 140335987144576 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W0707 10:34:24.285442 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.285622 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.285696 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.285751 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.285805 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.285861 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.285913 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.285962 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.286014 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.286062 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/weights] is not available in checkpoint\n",
            "W0707 10:34:24.286110 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/conv3/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.286162 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/conv3/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.286210 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/conv3/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.286262 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/conv3/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.286310 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/conv3/weights] is not available in checkpoint\n",
            "W0707 10:34:24.286358 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.286405 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.286452 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.286499 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.286546 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut/weights] is not available in checkpoint\n",
            "W0707 10:34:24.286606 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_2/bottleneck_v1/conv1/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.286659 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_2/bottleneck_v1/conv1/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.286707 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_2/bottleneck_v1/conv1/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.286755 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_2/bottleneck_v1/conv1/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.286803 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_2/bottleneck_v1/conv1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.286851 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_2/bottleneck_v1/conv2/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.286898 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_2/bottleneck_v1/conv2/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.286946 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_2/bottleneck_v1/conv2/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.286992 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_2/bottleneck_v1/conv2/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.287040 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_2/bottleneck_v1/conv2/weights] is not available in checkpoint\n",
            "W0707 10:34:24.287087 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_2/bottleneck_v1/conv3/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.287135 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_2/bottleneck_v1/conv3/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.287182 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_2/bottleneck_v1/conv3/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.287229 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_2/bottleneck_v1/conv3/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.287292 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_2/bottleneck_v1/conv3/weights] is not available in checkpoint\n",
            "W0707 10:34:24.287343 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_3/bottleneck_v1/conv1/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.287390 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_3/bottleneck_v1/conv1/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.287438 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_3/bottleneck_v1/conv1/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.287485 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_3/bottleneck_v1/conv1/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.287533 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_3/bottleneck_v1/conv1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.287580 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_3/bottleneck_v1/conv2/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.287646 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_3/bottleneck_v1/conv2/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.287696 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_3/bottleneck_v1/conv2/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.287745 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_3/bottleneck_v1/conv2/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.287794 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_3/bottleneck_v1/conv2/weights] is not available in checkpoint\n",
            "W0707 10:34:24.287843 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_3/bottleneck_v1/conv3/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.287891 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_3/bottleneck_v1/conv3/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.287939 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_3/bottleneck_v1/conv3/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.287997 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_3/bottleneck_v1/conv3/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.288047 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block1/unit_3/bottleneck_v1/conv3/weights] is not available in checkpoint\n",
            "W0707 10:34:24.288094 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/conv1/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.288143 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/conv1/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.288191 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/conv1/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.288239 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/conv1/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.288292 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/conv1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.288340 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/conv2/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.288388 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/conv2/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.288435 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/conv2/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.288483 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/conv2/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.288531 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/conv2/weights] is not available in checkpoint\n",
            "W0707 10:34:24.288578 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/conv3/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.288650 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/conv3/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.288699 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/conv3/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.288747 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/conv3/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.288796 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/conv3/weights] is not available in checkpoint\n",
            "W0707 10:34:24.288847 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.288894 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.288943 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.288991 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.289038 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut/weights] is not available in checkpoint\n",
            "W0707 10:34:24.289086 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_2/bottleneck_v1/conv1/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.289133 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_2/bottleneck_v1/conv1/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.289181 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_2/bottleneck_v1/conv1/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.289230 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_2/bottleneck_v1/conv1/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.289283 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_2/bottleneck_v1/conv1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.289332 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_2/bottleneck_v1/conv2/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.289380 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_2/bottleneck_v1/conv2/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.289428 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_2/bottleneck_v1/conv2/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.289494 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_2/bottleneck_v1/conv2/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.289545 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_2/bottleneck_v1/conv2/weights] is not available in checkpoint\n",
            "W0707 10:34:24.289618 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_2/bottleneck_v1/conv3/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.289670 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_2/bottleneck_v1/conv3/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.289719 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_2/bottleneck_v1/conv3/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.289768 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_2/bottleneck_v1/conv3/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.289837 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_2/bottleneck_v1/conv3/weights] is not available in checkpoint\n",
            "W0707 10:34:24.289890 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_3/bottleneck_v1/conv1/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.289942 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_3/bottleneck_v1/conv1/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.289993 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_3/bottleneck_v1/conv1/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.290044 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_3/bottleneck_v1/conv1/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.290095 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_3/bottleneck_v1/conv1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.290146 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_3/bottleneck_v1/conv2/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.290196 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_3/bottleneck_v1/conv2/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.290248 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_3/bottleneck_v1/conv2/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.290305 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_3/bottleneck_v1/conv2/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.290356 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_3/bottleneck_v1/conv2/weights] is not available in checkpoint\n",
            "W0707 10:34:24.290417 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_3/bottleneck_v1/conv3/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.290465 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_3/bottleneck_v1/conv3/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.290513 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_3/bottleneck_v1/conv3/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.290560 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_3/bottleneck_v1/conv3/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.290619 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_3/bottleneck_v1/conv3/weights] is not available in checkpoint\n",
            "W0707 10:34:24.290668 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_4/bottleneck_v1/conv1/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.290715 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_4/bottleneck_v1/conv1/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.290763 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_4/bottleneck_v1/conv1/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.345354 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_4/bottleneck_v1/conv1/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.345557 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_4/bottleneck_v1/conv1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.345712 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_4/bottleneck_v1/conv2/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.345801 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_4/bottleneck_v1/conv2/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.345875 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_4/bottleneck_v1/conv2/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.345945 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_4/bottleneck_v1/conv2/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.346013 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_4/bottleneck_v1/conv2/weights] is not available in checkpoint\n",
            "W0707 10:34:24.346081 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_4/bottleneck_v1/conv3/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.346148 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_4/bottleneck_v1/conv3/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.346214 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_4/bottleneck_v1/conv3/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.346302 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_4/bottleneck_v1/conv3/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.346375 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block2/unit_4/bottleneck_v1/conv3/weights] is not available in checkpoint\n",
            "W0707 10:34:24.346436 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/conv1/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.346491 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/conv1/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.346551 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/conv1/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.346640 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/conv1/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.346709 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/conv1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.346772 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/conv2/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.346834 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/conv2/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.346896 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/conv2/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.346957 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/conv2/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.347018 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/conv2/weights] is not available in checkpoint\n",
            "W0707 10:34:24.347077 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/conv3/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.347140 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/conv3/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.347203 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/conv3/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.347279 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/conv3/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.347347 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/conv3/weights] is not available in checkpoint\n",
            "W0707 10:34:24.347413 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.347481 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.347547 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.347629 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.347696 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut/weights] is not available in checkpoint\n",
            "W0707 10:34:24.347758 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_2/bottleneck_v1/conv1/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.347821 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_2/bottleneck_v1/conv1/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.347884 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_2/bottleneck_v1/conv1/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.347947 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_2/bottleneck_v1/conv1/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.348029 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_2/bottleneck_v1/conv1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.348095 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_2/bottleneck_v1/conv2/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.348160 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_2/bottleneck_v1/conv2/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.348227 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_2/bottleneck_v1/conv2/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.348314 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_2/bottleneck_v1/conv2/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.348378 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_2/bottleneck_v1/conv2/weights] is not available in checkpoint\n",
            "W0707 10:34:24.348440 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_2/bottleneck_v1/conv3/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.348504 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_2/bottleneck_v1/conv3/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.348567 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_2/bottleneck_v1/conv3/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.348657 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_2/bottleneck_v1/conv3/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.348724 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_2/bottleneck_v1/conv3/weights] is not available in checkpoint\n",
            "W0707 10:34:24.348790 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_3/bottleneck_v1/conv1/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.348855 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_3/bottleneck_v1/conv1/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.348919 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_3/bottleneck_v1/conv1/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.348983 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_3/bottleneck_v1/conv1/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.349046 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_3/bottleneck_v1/conv1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.349111 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_3/bottleneck_v1/conv2/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.349176 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_3/bottleneck_v1/conv2/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.349241 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_3/bottleneck_v1/conv2/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.349319 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_3/bottleneck_v1/conv2/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.349383 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_3/bottleneck_v1/conv2/weights] is not available in checkpoint\n",
            "W0707 10:34:24.349445 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_3/bottleneck_v1/conv3/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.349506 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_3/bottleneck_v1/conv3/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.349567 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_3/bottleneck_v1/conv3/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.349650 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_3/bottleneck_v1/conv3/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.349713 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_3/bottleneck_v1/conv3/weights] is not available in checkpoint\n",
            "W0707 10:34:24.349773 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_4/bottleneck_v1/conv1/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.349823 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_4/bottleneck_v1/conv1/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.349874 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_4/bottleneck_v1/conv1/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.349927 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_4/bottleneck_v1/conv1/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.349978 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_4/bottleneck_v1/conv1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.350027 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_4/bottleneck_v1/conv2/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.350075 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_4/bottleneck_v1/conv2/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.350123 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_4/bottleneck_v1/conv2/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.350171 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_4/bottleneck_v1/conv2/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.350219 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_4/bottleneck_v1/conv2/weights] is not available in checkpoint\n",
            "W0707 10:34:24.350275 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_4/bottleneck_v1/conv3/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.350326 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_4/bottleneck_v1/conv3/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.350375 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_4/bottleneck_v1/conv3/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.350423 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_4/bottleneck_v1/conv3/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.350471 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_4/bottleneck_v1/conv3/weights] is not available in checkpoint\n",
            "W0707 10:34:24.350518 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_5/bottleneck_v1/conv1/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.350565 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_5/bottleneck_v1/conv1/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.350630 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_5/bottleneck_v1/conv1/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.350678 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_5/bottleneck_v1/conv1/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.350725 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_5/bottleneck_v1/conv1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.350773 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_5/bottleneck_v1/conv2/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.350820 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_5/bottleneck_v1/conv2/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.350868 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_5/bottleneck_v1/conv2/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.350917 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_5/bottleneck_v1/conv2/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.350964 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_5/bottleneck_v1/conv2/weights] is not available in checkpoint\n",
            "W0707 10:34:24.351011 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_5/bottleneck_v1/conv3/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.351061 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_5/bottleneck_v1/conv3/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.351109 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_5/bottleneck_v1/conv3/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.351161 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_5/bottleneck_v1/conv3/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.351216 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_5/bottleneck_v1/conv3/weights] is not available in checkpoint\n",
            "W0707 10:34:24.351274 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_6/bottleneck_v1/conv1/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.351327 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_6/bottleneck_v1/conv1/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.351377 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_6/bottleneck_v1/conv1/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.351427 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_6/bottleneck_v1/conv1/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.351476 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_6/bottleneck_v1/conv1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.351528 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_6/bottleneck_v1/conv2/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.351580 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_6/bottleneck_v1/conv2/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.351649 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_6/bottleneck_v1/conv2/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.351704 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_6/bottleneck_v1/conv2/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.351755 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_6/bottleneck_v1/conv2/weights] is not available in checkpoint\n",
            "W0707 10:34:24.351810 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_6/bottleneck_v1/conv3/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.351865 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_6/bottleneck_v1/conv3/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.351919 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_6/bottleneck_v1/conv3/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.351971 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_6/bottleneck_v1/conv3/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.352022 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block3/unit_6/bottleneck_v1/conv3/weights] is not available in checkpoint\n",
            "W0707 10:34:24.352072 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.352122 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.352172 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.352222 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.352282 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.352335 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/conv2/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.352385 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/conv2/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.352436 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/conv2/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.352487 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/conv2/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.352539 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/conv2/weights] is not available in checkpoint\n",
            "W0707 10:34:24.352607 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/conv3/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.352663 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/conv3/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.352715 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/conv3/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.352772 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/conv3/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.352826 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/conv3/weights] is not available in checkpoint\n",
            "W0707 10:34:24.352877 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.352931 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.352983 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.353033 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.353086 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut/weights] is not available in checkpoint\n",
            "W0707 10:34:24.353516 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_2/bottleneck_v1/conv1/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.353633 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_2/bottleneck_v1/conv1/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.353703 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_2/bottleneck_v1/conv1/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.353768 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_2/bottleneck_v1/conv1/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.353831 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_2/bottleneck_v1/conv1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.353888 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_2/bottleneck_v1/conv2/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.353947 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_2/bottleneck_v1/conv2/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.354002 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_2/bottleneck_v1/conv2/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.354060 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_2/bottleneck_v1/conv2/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.354115 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_2/bottleneck_v1/conv2/weights] is not available in checkpoint\n",
            "W0707 10:34:24.354171 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_2/bottleneck_v1/conv3/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.354224 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_2/bottleneck_v1/conv3/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.354286 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_2/bottleneck_v1/conv3/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.354339 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_2/bottleneck_v1/conv3/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.354391 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_2/bottleneck_v1/conv3/weights] is not available in checkpoint\n",
            "W0707 10:34:24.354444 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_3/bottleneck_v1/conv1/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.354495 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_3/bottleneck_v1/conv1/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.354548 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_3/bottleneck_v1/conv1/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.354613 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_3/bottleneck_v1/conv1/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.354667 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_3/bottleneck_v1/conv1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.354721 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_3/bottleneck_v1/conv2/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.354773 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_3/bottleneck_v1/conv2/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.354828 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_3/bottleneck_v1/conv2/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.354882 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_3/bottleneck_v1/conv2/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.354934 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_3/bottleneck_v1/conv2/weights] is not available in checkpoint\n",
            "W0707 10:34:24.354989 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_3/bottleneck_v1/conv3/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.355040 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_3/bottleneck_v1/conv3/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.355176 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_3/bottleneck_v1/conv3/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.355238 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_3/bottleneck_v1/conv3/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.355305 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/block4/unit_3/bottleneck_v1/conv3/weights] is not available in checkpoint\n",
            "W0707 10:34:24.449717 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/conv1/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.449822 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/conv1/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.449896 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/conv1/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.449967 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/conv1/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.450029 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/conv1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.450090 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/bottom_up_block5/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.450150 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/bottom_up_block5/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.450207 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/bottom_up_block5/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.450287 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/bottom_up_block5/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.450352 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/bottom_up_block5/weights] is not available in checkpoint\n",
            "W0707 10:34:24.450411 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/bottom_up_block6/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.450467 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/bottom_up_block6/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.450525 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/bottom_up_block6/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.450583 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/bottom_up_block6/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.450693 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/bottom_up_block6/weights] is not available in checkpoint\n",
            "W0707 10:34:24.450753 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/projection_1/biases] is not available in checkpoint\n",
            "W0707 10:34:24.450810 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/projection_1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.450869 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/projection_2/biases] is not available in checkpoint\n",
            "W0707 10:34:24.450924 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/projection_2/weights] is not available in checkpoint\n",
            "W0707 10:34:24.450982 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/projection_3/biases] is not available in checkpoint\n",
            "W0707 10:34:24.451037 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/projection_3/weights] is not available in checkpoint\n",
            "W0707 10:34:24.451094 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/smoothing_1/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.451152 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/smoothing_1/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.451208 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/smoothing_1/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.451274 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/smoothing_1/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.451335 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/smoothing_1/weights] is not available in checkpoint\n",
            "W0707 10:34:24.451392 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/smoothing_2/BatchNorm/beta] is not available in checkpoint\n",
            "W0707 10:34:24.451456 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/smoothing_2/BatchNorm/gamma] is not available in checkpoint\n",
            "W0707 10:34:24.451521 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/smoothing_2/BatchNorm/moving_mean] is not available in checkpoint\n",
            "W0707 10:34:24.451585 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/smoothing_2/BatchNorm/moving_variance] is not available in checkpoint\n",
            "W0707 10:34:24.451667 140335987144576 variables_helper.py:156] Variable [resnet_v1_50/fpn/smoothing_2/weights] is not available in checkpoint\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0707 10:34:29.127306 140335987144576 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "I0707 10:34:29.128571 140335987144576 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0707 10:34:31.117682 140335987144576 monitored_session.py:240] Graph was finalized.\n",
            "2020-07-07 10:34:31.118110: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
            "2020-07-07 10:34:31.130227: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000179999 Hz\n",
            "2020-07-07 10:34:31.130523: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x105f24c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-07-07 10:34:31.130558: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-07-07 10:34:31.136291: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-07-07 10:34:31.305129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:34:31.305857: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x105f2300 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-07-07 10:34:31.305895: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-07-07 10:34:31.306871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:34:31.307447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-07-07 10:34:31.307811: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-07-07 10:34:31.581697: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-07-07 10:34:31.711361: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-07-07 10:34:31.740033: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-07-07 10:34:32.032157: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-07-07 10:34:32.063820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-07-07 10:34:32.599787: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-07-07 10:34:32.600035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:34:32.600956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:34:32.601641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-07-07 10:34:32.606291: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-07-07 10:34:32.607832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-07-07 10:34:32.607867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2020-07-07 10:34:32.607882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2020-07-07 10:34:32.609064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:34:32.609684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:34:32.610223: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-07-07 10:34:32.610270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0707 10:34:41.065767 140335987144576 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0707 10:34:41.263192 140335987144576 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into training/model.ckpt.\n",
            "I0707 10:34:46.799550 140335987144576 basic_session_run_hooks.py:606] Saving checkpoints for 0 into training/model.ckpt.\n",
            "2020-07-07 10:34:53.623245: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-07-07 10:34:56.113183: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "INFO:tensorflow:loss = 5.747773, step = 0\n",
            "I0707 10:35:02.340938 140335987144576 basic_session_run_hooks.py:262] loss = 5.747773, step = 0\n",
            "INFO:tensorflow:global_step/sec: 1.55294\n",
            "I0707 10:36:06.734291 140335987144576 basic_session_run_hooks.py:692] global_step/sec: 1.55294\n",
            "INFO:tensorflow:loss = 5.18055, step = 100 (64.395 sec)\n",
            "I0707 10:36:06.735641 140335987144576 basic_session_run_hooks.py:260] loss = 5.18055, step = 100 (64.395 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.64277\n",
            "I0707 10:37:07.606946 140335987144576 basic_session_run_hooks.py:692] global_step/sec: 1.64277\n",
            "INFO:tensorflow:loss = 4.9491544, step = 200 (60.872 sec)\n",
            "I0707 10:37:07.608121 140335987144576 basic_session_run_hooks.py:260] loss = 4.9491544, step = 200 (60.872 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.64599\n",
            "I0707 10:38:08.360553 140335987144576 basic_session_run_hooks.py:692] global_step/sec: 1.64599\n",
            "INFO:tensorflow:loss = 4.8971486, step = 300 (60.754 sec)\n",
            "I0707 10:38:08.361686 140335987144576 basic_session_run_hooks.py:260] loss = 4.8971486, step = 300 (60.754 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.65602\n",
            "I0707 10:39:08.746130 140335987144576 basic_session_run_hooks.py:692] global_step/sec: 1.65602\n",
            "INFO:tensorflow:loss = 4.9835668, step = 400 (60.386 sec)\n",
            "I0707 10:39:08.747206 140335987144576 basic_session_run_hooks.py:260] loss = 4.9835668, step = 400 (60.386 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.64987\n",
            "I0707 10:40:09.356873 140335987144576 basic_session_run_hooks.py:692] global_step/sec: 1.64987\n",
            "INFO:tensorflow:loss = 4.693782, step = 500 (60.611 sec)\n",
            "I0707 10:40:09.357911 140335987144576 basic_session_run_hooks.py:260] loss = 4.693782, step = 500 (60.611 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 600 into training/model.ckpt.\n",
            "I0707 10:41:09.307251 140335987144576 basic_session_run_hooks.py:606] Saving checkpoints for 600 into training/model.ckpt.\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7fa20a064e80>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "W0707 10:41:10.679540 140335987144576 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7fa20a064e80>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7fa2169529d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "W0707 10:41:10.845542 140335987144576 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7fa2169529d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0707 10:41:11.478176 140335987144576 estimator.py:1148] Calling model_fn.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/eval_util.py:830: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0707 10:41:14.906908 140335987144576 deprecation.py:323] From /content/models/research/object_detection/eval_util.py:830: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/visualization_utils.py:618: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0707 10:41:15.086265 140335987144576 deprecation.py:323] From /content/models/research/object_detection/utils/visualization_utils.py:618: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0707 10:41:15.669053 140335987144576 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-07-07T10:41:15Z\n",
            "I0707 10:41:15.684118 140335987144576 evaluation.py:255] Starting evaluation at 2020-07-07T10:41:15Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0707 10:41:16.203386 140335987144576 monitored_session.py:240] Graph was finalized.\n",
            "2020-07-07 10:41:16.204365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:16.204783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-07-07 10:41:16.204911: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-07-07 10:41:16.204948: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-07-07 10:41:16.204974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-07-07 10:41:16.204995: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-07-07 10:41:16.205016: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-07-07 10:41:16.205036: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-07-07 10:41:16.205057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-07-07 10:41:16.205143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:16.205521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:16.205837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-07-07 10:41:16.205884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-07-07 10:41:16.205897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2020-07-07 10:41:16.205906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2020-07-07 10:41:16.205999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:16.206365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:16.206698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-600\n",
            "I0707 10:41:16.207642 140335987144576 saver.py:1284] Restoring parameters from training/model.ckpt-600\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0707 10:41:17.382277 140335987144576 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0707 10:41:17.517493 140335987144576 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 30 images.\n",
            "I0707 10:41:21.768398 140332278482688 coco_evaluation.py:237] Performing evaluation on 30 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0707 10:41:21.768847 140332278482688 coco_tools.py:116] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.00s)\n",
            "I0707 10:41:21.770354 140332278482688 coco_tools.py:138] DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.14s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.438\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.161\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.022\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.220\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.371\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.503\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.637\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.450\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.669\n",
            "INFO:tensorflow:Finished evaluation at 2020-07-07-10:41:22\n",
            "I0707 10:41:22.544949 140335987144576 evaluation.py:275] Finished evaluation at 2020-07-07-10:41:22\n",
            "INFO:tensorflow:Saving dict for global step 600: DetectionBoxes_Precision/mAP = 0.20182338, DetectionBoxes_Precision/mAP (large) = 0.21974614, DetectionBoxes_Precision/mAP (medium) = 0.022121822, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.4376704, DetectionBoxes_Precision/mAP@.75IOU = 0.16076794, DetectionBoxes_Recall/AR@1 = 0.37055555, DetectionBoxes_Recall/AR@10 = 0.50333333, DetectionBoxes_Recall/AR@100 = 0.63722223, DetectionBoxes_Recall/AR@100 (large) = 0.66875, DetectionBoxes_Recall/AR@100 (medium) = 0.45, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 0.6148609, Loss/localization_loss = 0.36511004, Loss/regularization_loss = 4.3242407, Loss/total_loss = 5.304211, global_step = 600, learning_rate = 0.0213331, loss = 5.304211\n",
            "I0707 10:41:22.545237 140335987144576 estimator.py:2049] Saving dict for global step 600: DetectionBoxes_Precision/mAP = 0.20182338, DetectionBoxes_Precision/mAP (large) = 0.21974614, DetectionBoxes_Precision/mAP (medium) = 0.022121822, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.4376704, DetectionBoxes_Precision/mAP@.75IOU = 0.16076794, DetectionBoxes_Recall/AR@1 = 0.37055555, DetectionBoxes_Recall/AR@10 = 0.50333333, DetectionBoxes_Recall/AR@100 = 0.63722223, DetectionBoxes_Recall/AR@100 (large) = 0.66875, DetectionBoxes_Recall/AR@100 (medium) = 0.45, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 0.6148609, Loss/localization_loss = 0.36511004, Loss/regularization_loss = 4.3242407, Loss/total_loss = 5.304211, global_step = 600, learning_rate = 0.0213331, loss = 5.304211\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 600: training/model.ckpt-600\n",
            "I0707 10:41:23.585482 140335987144576 estimator.py:2109] Saving 'checkpoint_path' summary for global step 600: training/model.ckpt-600\n",
            "INFO:tensorflow:Performing the final export in the end of training.\n",
            "I0707 10:41:23.586254 140335987144576 exporter.py:410] Performing the final export in the end of training.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0707 10:41:23.815049 140335987144576 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0707 10:41:27.303544 140335987144576 estimator.py:1150] Done calling model_fn.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W0707 10:41:27.303845 140335987144576 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "I0707 10:41:27.304465 140335987144576 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "I0707 10:41:27.304599 140335987144576 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['tensorflow/serving/predict', 'serving_default']\n",
            "I0707 10:41:27.304688 140335987144576 export_utils.py:170] Signatures INCLUDED in export for Predict: ['tensorflow/serving/predict', 'serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "I0707 10:41:27.304755 140335987144576 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "I0707 10:41:27.304819 140335987144576 export_utils.py:170] Signatures INCLUDED in export for Eval: None\n",
            "2020-07-07 10:41:27.305337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:27.305757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-07-07 10:41:27.305845: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-07-07 10:41:27.305878: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-07-07 10:41:27.305902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-07-07 10:41:27.305928: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-07-07 10:41:27.305954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-07-07 10:41:27.305974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-07-07 10:41:27.305995: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-07-07 10:41:27.306082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:27.306452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:27.306767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-07-07 10:41:27.306807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-07-07 10:41:27.306820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2020-07-07 10:41:27.306830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2020-07-07 10:41:27.306918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:27.307280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:27.307606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-600\n",
            "I0707 10:41:27.309760 140335987144576 saver.py:1284] Restoring parameters from training/model.ckpt-600\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "I0707 10:41:27.926844 140335987144576 builder_impl.py:665] Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "I0707 10:41:27.927058 140335987144576 builder_impl.py:460] No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: training/export/Servo/temp-b'1594118483'/saved_model.pb\n",
            "I0707 10:41:29.022497 140335987144576 builder_impl.py:425] SavedModel written to: training/export/Servo/temp-b'1594118483'/saved_model.pb\n",
            "INFO:tensorflow:Loss for final step: 4.6825376.\n",
            "I0707 10:41:29.250702 140335987144576 estimator.py:371] Loss for final step: 4.6825376.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP-tUdtnRybs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "192d014e-35fd-4aea-9f1e-7db6f838f625"
      },
      "source": [
        "!ls {model_dir}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\t\t\t\t     model.ckpt-0.index\n",
            "eval_0\t\t\t\t\t     model.ckpt-0.meta\n",
            "events.out.tfevents.1594118069.32353572f03c  model.ckpt-600.data-00000-of-00001\n",
            "export\t\t\t\t\t     model.ckpt-600.index\n",
            "graph.pbtxt\t\t\t\t     model.ckpt-600.meta\n",
            "model.ckpt-0.data-00000-of-00001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmSESMetj1sa",
        "colab_type": "text"
      },
      "source": [
        "## Exportar o modelo para realizar as inferências"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHoP90pUyKSq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f04ca57a-7518-4ab0-f611-1a61ae7bbe35"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "output_directory = './fine_tuned_model'\n",
        "\n",
        "lst = os.listdir(model_dir)\n",
        "lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n",
        "steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
        "last_model = lst[steps.argmax()].replace('.meta', '')\n",
        "\n",
        "last_model_path = os.path.join(model_dir, last_model)\n",
        "print(last_model_path)\n",
        "!python /content/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --output_directory={output_directory} \\\n",
        "    --trained_checkpoint_prefix={last_model_path}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training/model.ckpt-600\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W0707 10:41:38.675281 139766366193536 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/post_processing.py:583: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0707 10:41:41.829801 139766366193536 deprecation.py:323] From /content/models/research/object_detection/core/post_processing.py:583: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:474: get_or_create_global_step (from tf_slim.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "W0707 10:41:42.153623 139766366193536 deprecation.py:323] From /content/models/research/object_detection/exporter.py:474: get_or_create_global_step (from tf_slim.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:653: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "W0707 10:41:42.156558 139766366193536 deprecation.py:323] From /content/models/research/object_detection/exporter.py:653: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "W0707 10:41:42.157113 139766366193536 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "177 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              0\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   name\n",
            "-account_type_regexes       _trainable_variables\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     params\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "param: Number of parameters (in the Variable).\n",
            "\n",
            "Profile:\n",
            "node name | # parameters\n",
            "_TFProfRoot (--/31.55m params)\n",
            "  FeatureExtractor (--/26.73m params)\n",
            "    FeatureExtractor/resnet_v1_50 (--/26.73m params)\n",
            "      FeatureExtractor/resnet_v1_50/block1 (--/212.99k params)\n",
            "        FeatureExtractor/resnet_v1_50/block1/unit_1 (--/73.73k params)\n",
            "          FeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1 (--/73.73k params)\n",
            "            FeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1/conv1 (--/4.10k params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/weights (1x1x64x64, 4.10k/4.10k params)\n",
            "            FeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1/conv2 (--/36.86k params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/weights (3x3x64x64, 36.86k/36.86k params)\n",
            "            FeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1/conv3 (--/16.38k params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1/conv3/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1/conv3/weights (1x1x64x256, 16.38k/16.38k params)\n",
            "            FeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut (--/16.38k params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut/weights (1x1x64x256, 16.38k/16.38k params)\n",
            "        FeatureExtractor/resnet_v1_50/block1/unit_2 (--/69.63k params)\n",
            "          FeatureExtractor/resnet_v1_50/block1/unit_2/bottleneck_v1 (--/69.63k params)\n",
            "            FeatureExtractor/resnet_v1_50/block1/unit_2/bottleneck_v1/conv1 (--/16.38k params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_2/bottleneck_v1/conv1/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_2/bottleneck_v1/conv1/weights (1x1x256x64, 16.38k/16.38k params)\n",
            "            FeatureExtractor/resnet_v1_50/block1/unit_2/bottleneck_v1/conv2 (--/36.86k params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_2/bottleneck_v1/conv2/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_2/bottleneck_v1/conv2/weights (3x3x64x64, 36.86k/36.86k params)\n",
            "            FeatureExtractor/resnet_v1_50/block1/unit_2/bottleneck_v1/conv3 (--/16.38k params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_2/bottleneck_v1/conv3/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_2/bottleneck_v1/conv3/weights (1x1x64x256, 16.38k/16.38k params)\n",
            "        FeatureExtractor/resnet_v1_50/block1/unit_3 (--/69.63k params)\n",
            "          FeatureExtractor/resnet_v1_50/block1/unit_3/bottleneck_v1 (--/69.63k params)\n",
            "            FeatureExtractor/resnet_v1_50/block1/unit_3/bottleneck_v1/conv1 (--/16.38k params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_3/bottleneck_v1/conv1/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_3/bottleneck_v1/conv1/weights (1x1x256x64, 16.38k/16.38k params)\n",
            "            FeatureExtractor/resnet_v1_50/block1/unit_3/bottleneck_v1/conv2 (--/36.86k params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_3/bottleneck_v1/conv2/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_3/bottleneck_v1/conv2/weights (3x3x64x64, 36.86k/36.86k params)\n",
            "            FeatureExtractor/resnet_v1_50/block1/unit_3/bottleneck_v1/conv3 (--/16.38k params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_3/bottleneck_v1/conv3/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block1/unit_3/bottleneck_v1/conv3/weights (1x1x64x256, 16.38k/16.38k params)\n",
            "      FeatureExtractor/resnet_v1_50/block2 (--/1.21m params)\n",
            "        FeatureExtractor/resnet_v1_50/block2/unit_1 (--/376.83k params)\n",
            "          FeatureExtractor/resnet_v1_50/block2/unit_1/bottleneck_v1 (--/376.83k params)\n",
            "            FeatureExtractor/resnet_v1_50/block2/unit_1/bottleneck_v1/conv1 (--/32.77k params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_1/bottleneck_v1/conv1/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_1/bottleneck_v1/conv1/weights (1x1x256x128, 32.77k/32.77k params)\n",
            "            FeatureExtractor/resnet_v1_50/block2/unit_1/bottleneck_v1/conv2 (--/147.46k params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_1/bottleneck_v1/conv2/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_1/bottleneck_v1/conv2/weights (3x3x128x128, 147.46k/147.46k params)\n",
            "            FeatureExtractor/resnet_v1_50/block2/unit_1/bottleneck_v1/conv3 (--/65.54k params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_1/bottleneck_v1/conv3/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_1/bottleneck_v1/conv3/weights (1x1x128x512, 65.54k/65.54k params)\n",
            "            FeatureExtractor/resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut (--/131.07k params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut/weights (1x1x256x512, 131.07k/131.07k params)\n",
            "        FeatureExtractor/resnet_v1_50/block2/unit_2 (--/278.53k params)\n",
            "          FeatureExtractor/resnet_v1_50/block2/unit_2/bottleneck_v1 (--/278.53k params)\n",
            "            FeatureExtractor/resnet_v1_50/block2/unit_2/bottleneck_v1/conv1 (--/65.54k params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_2/bottleneck_v1/conv1/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_2/bottleneck_v1/conv1/weights (1x1x512x128, 65.54k/65.54k params)\n",
            "            FeatureExtractor/resnet_v1_50/block2/unit_2/bottleneck_v1/conv2 (--/147.46k params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_2/bottleneck_v1/conv2/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_2/bottleneck_v1/conv2/weights (3x3x128x128, 147.46k/147.46k params)\n",
            "            FeatureExtractor/resnet_v1_50/block2/unit_2/bottleneck_v1/conv3 (--/65.54k params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_2/bottleneck_v1/conv3/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_2/bottleneck_v1/conv3/weights (1x1x128x512, 65.54k/65.54k params)\n",
            "        FeatureExtractor/resnet_v1_50/block2/unit_3 (--/278.53k params)\n",
            "          FeatureExtractor/resnet_v1_50/block2/unit_3/bottleneck_v1 (--/278.53k params)\n",
            "            FeatureExtractor/resnet_v1_50/block2/unit_3/bottleneck_v1/conv1 (--/65.54k params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_3/bottleneck_v1/conv1/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_3/bottleneck_v1/conv1/weights (1x1x512x128, 65.54k/65.54k params)\n",
            "            FeatureExtractor/resnet_v1_50/block2/unit_3/bottleneck_v1/conv2 (--/147.46k params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_3/bottleneck_v1/conv2/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_3/bottleneck_v1/conv2/weights (3x3x128x128, 147.46k/147.46k params)\n",
            "            FeatureExtractor/resnet_v1_50/block2/unit_3/bottleneck_v1/conv3 (--/65.54k params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_3/bottleneck_v1/conv3/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_3/bottleneck_v1/conv3/weights (1x1x128x512, 65.54k/65.54k params)\n",
            "        FeatureExtractor/resnet_v1_50/block2/unit_4 (--/278.53k params)\n",
            "          FeatureExtractor/resnet_v1_50/block2/unit_4/bottleneck_v1 (--/278.53k params)\n",
            "            FeatureExtractor/resnet_v1_50/block2/unit_4/bottleneck_v1/conv1 (--/65.54k params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_4/bottleneck_v1/conv1/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_4/bottleneck_v1/conv1/weights (1x1x512x128, 65.54k/65.54k params)\n",
            "            FeatureExtractor/resnet_v1_50/block2/unit_4/bottleneck_v1/conv2 (--/147.46k params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_4/bottleneck_v1/conv2/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_4/bottleneck_v1/conv2/weights (3x3x128x128, 147.46k/147.46k params)\n",
            "            FeatureExtractor/resnet_v1_50/block2/unit_4/bottleneck_v1/conv3 (--/65.54k params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_4/bottleneck_v1/conv3/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block2/unit_4/bottleneck_v1/conv3/weights (1x1x128x512, 65.54k/65.54k params)\n",
            "      FeatureExtractor/resnet_v1_50/block3 (--/7.08m params)\n",
            "        FeatureExtractor/resnet_v1_50/block3/unit_1 (--/1.51m params)\n",
            "          FeatureExtractor/resnet_v1_50/block3/unit_1/bottleneck_v1 (--/1.51m params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_1/bottleneck_v1/conv1 (--/131.07k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_1/bottleneck_v1/conv1/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_1/bottleneck_v1/conv1/weights (1x1x512x256, 131.07k/131.07k params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_1/bottleneck_v1/conv2 (--/589.82k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_1/bottleneck_v1/conv2/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_1/bottleneck_v1/conv2/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_1/bottleneck_v1/conv3 (--/262.14k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_1/bottleneck_v1/conv3/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_1/bottleneck_v1/conv3/weights (1x1x256x1024, 262.14k/262.14k params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut (--/524.29k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut/weights (1x1x512x1024, 524.29k/524.29k params)\n",
            "        FeatureExtractor/resnet_v1_50/block3/unit_2 (--/1.11m params)\n",
            "          FeatureExtractor/resnet_v1_50/block3/unit_2/bottleneck_v1 (--/1.11m params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_2/bottleneck_v1/conv1 (--/262.14k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_2/bottleneck_v1/conv1/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_2/bottleneck_v1/conv1/weights (1x1x1024x256, 262.14k/262.14k params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_2/bottleneck_v1/conv2 (--/589.82k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_2/bottleneck_v1/conv2/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_2/bottleneck_v1/conv2/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_2/bottleneck_v1/conv3 (--/262.14k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_2/bottleneck_v1/conv3/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_2/bottleneck_v1/conv3/weights (1x1x256x1024, 262.14k/262.14k params)\n",
            "        FeatureExtractor/resnet_v1_50/block3/unit_3 (--/1.11m params)\n",
            "          FeatureExtractor/resnet_v1_50/block3/unit_3/bottleneck_v1 (--/1.11m params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_3/bottleneck_v1/conv1 (--/262.14k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_3/bottleneck_v1/conv1/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_3/bottleneck_v1/conv1/weights (1x1x1024x256, 262.14k/262.14k params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_3/bottleneck_v1/conv2 (--/589.82k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_3/bottleneck_v1/conv2/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_3/bottleneck_v1/conv2/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_3/bottleneck_v1/conv3 (--/262.14k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_3/bottleneck_v1/conv3/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_3/bottleneck_v1/conv3/weights (1x1x256x1024, 262.14k/262.14k params)\n",
            "        FeatureExtractor/resnet_v1_50/block3/unit_4 (--/1.11m params)\n",
            "          FeatureExtractor/resnet_v1_50/block3/unit_4/bottleneck_v1 (--/1.11m params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_4/bottleneck_v1/conv1 (--/262.14k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_4/bottleneck_v1/conv1/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_4/bottleneck_v1/conv1/weights (1x1x1024x256, 262.14k/262.14k params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_4/bottleneck_v1/conv2 (--/589.82k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_4/bottleneck_v1/conv2/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_4/bottleneck_v1/conv2/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_4/bottleneck_v1/conv3 (--/262.14k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_4/bottleneck_v1/conv3/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_4/bottleneck_v1/conv3/weights (1x1x256x1024, 262.14k/262.14k params)\n",
            "        FeatureExtractor/resnet_v1_50/block3/unit_5 (--/1.11m params)\n",
            "          FeatureExtractor/resnet_v1_50/block3/unit_5/bottleneck_v1 (--/1.11m params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_5/bottleneck_v1/conv1 (--/262.14k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_5/bottleneck_v1/conv1/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_5/bottleneck_v1/conv1/weights (1x1x1024x256, 262.14k/262.14k params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_5/bottleneck_v1/conv2 (--/589.82k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_5/bottleneck_v1/conv2/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_5/bottleneck_v1/conv2/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_5/bottleneck_v1/conv3 (--/262.14k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_5/bottleneck_v1/conv3/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_5/bottleneck_v1/conv3/weights (1x1x256x1024, 262.14k/262.14k params)\n",
            "        FeatureExtractor/resnet_v1_50/block3/unit_6 (--/1.11m params)\n",
            "          FeatureExtractor/resnet_v1_50/block3/unit_6/bottleneck_v1 (--/1.11m params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_6/bottleneck_v1/conv1 (--/262.14k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_6/bottleneck_v1/conv1/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_6/bottleneck_v1/conv1/weights (1x1x1024x256, 262.14k/262.14k params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_6/bottleneck_v1/conv2 (--/589.82k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_6/bottleneck_v1/conv2/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_6/bottleneck_v1/conv2/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "            FeatureExtractor/resnet_v1_50/block3/unit_6/bottleneck_v1/conv3 (--/262.14k params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_6/bottleneck_v1/conv3/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block3/unit_6/bottleneck_v1/conv3/weights (1x1x256x1024, 262.14k/262.14k params)\n",
            "      FeatureExtractor/resnet_v1_50/block4 (--/14.94m params)\n",
            "        FeatureExtractor/resnet_v1_50/block4/unit_1 (--/6.03m params)\n",
            "          FeatureExtractor/resnet_v1_50/block4/unit_1/bottleneck_v1 (--/6.03m params)\n",
            "            FeatureExtractor/resnet_v1_50/block4/unit_1/bottleneck_v1/conv1 (--/524.29k params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/weights (1x1x1024x512, 524.29k/524.29k params)\n",
            "            FeatureExtractor/resnet_v1_50/block4/unit_1/bottleneck_v1/conv2 (--/2.36m params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_1/bottleneck_v1/conv2/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_1/bottleneck_v1/conv2/weights (3x3x512x512, 2.36m/2.36m params)\n",
            "            FeatureExtractor/resnet_v1_50/block4/unit_1/bottleneck_v1/conv3 (--/1.05m params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_1/bottleneck_v1/conv3/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_1/bottleneck_v1/conv3/weights (1x1x512x2048, 1.05m/1.05m params)\n",
            "            FeatureExtractor/resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut (--/2.10m params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut/weights (1x1x1024x2048, 2.10m/2.10m params)\n",
            "        FeatureExtractor/resnet_v1_50/block4/unit_2 (--/4.46m params)\n",
            "          FeatureExtractor/resnet_v1_50/block4/unit_2/bottleneck_v1 (--/4.46m params)\n",
            "            FeatureExtractor/resnet_v1_50/block4/unit_2/bottleneck_v1/conv1 (--/1.05m params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_2/bottleneck_v1/conv1/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_2/bottleneck_v1/conv1/weights (1x1x2048x512, 1.05m/1.05m params)\n",
            "            FeatureExtractor/resnet_v1_50/block4/unit_2/bottleneck_v1/conv2 (--/2.36m params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_2/bottleneck_v1/conv2/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_2/bottleneck_v1/conv2/weights (3x3x512x512, 2.36m/2.36m params)\n",
            "            FeatureExtractor/resnet_v1_50/block4/unit_2/bottleneck_v1/conv3 (--/1.05m params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_2/bottleneck_v1/conv3/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_2/bottleneck_v1/conv3/weights (1x1x512x2048, 1.05m/1.05m params)\n",
            "        FeatureExtractor/resnet_v1_50/block4/unit_3 (--/4.46m params)\n",
            "          FeatureExtractor/resnet_v1_50/block4/unit_3/bottleneck_v1 (--/4.46m params)\n",
            "            FeatureExtractor/resnet_v1_50/block4/unit_3/bottleneck_v1/conv1 (--/1.05m params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_3/bottleneck_v1/conv1/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_3/bottleneck_v1/conv1/weights (1x1x2048x512, 1.05m/1.05m params)\n",
            "            FeatureExtractor/resnet_v1_50/block4/unit_3/bottleneck_v1/conv2 (--/2.36m params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_3/bottleneck_v1/conv2/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_3/bottleneck_v1/conv2/weights (3x3x512x512, 2.36m/2.36m params)\n",
            "            FeatureExtractor/resnet_v1_50/block4/unit_3/bottleneck_v1/conv3 (--/1.05m params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_3/bottleneck_v1/conv3/BatchNorm (--/0 params)\n",
            "              FeatureExtractor/resnet_v1_50/block4/unit_3/bottleneck_v1/conv3/weights (1x1x512x2048, 1.05m/1.05m params)\n",
            "      FeatureExtractor/resnet_v1_50/conv1 (--/9.41k params)\n",
            "        FeatureExtractor/resnet_v1_50/conv1/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/resnet_v1_50/conv1/weights (7x7x3x64, 9.41k/9.41k params)\n",
            "      FeatureExtractor/resnet_v1_50/fpn (--/3.28m params)\n",
            "        FeatureExtractor/resnet_v1_50/fpn/bottom_up_block5 (--/589.82k params)\n",
            "          FeatureExtractor/resnet_v1_50/fpn/bottom_up_block5/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/resnet_v1_50/fpn/bottom_up_block5/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "        FeatureExtractor/resnet_v1_50/fpn/bottom_up_block6 (--/589.82k params)\n",
            "          FeatureExtractor/resnet_v1_50/fpn/bottom_up_block6/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/resnet_v1_50/fpn/bottom_up_block6/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "        FeatureExtractor/resnet_v1_50/fpn/projection_1 (--/131.33k params)\n",
            "          FeatureExtractor/resnet_v1_50/fpn/projection_1/biases (256, 256/256 params)\n",
            "          FeatureExtractor/resnet_v1_50/fpn/projection_1/weights (1x1x512x256, 131.07k/131.07k params)\n",
            "        FeatureExtractor/resnet_v1_50/fpn/projection_2 (--/262.40k params)\n",
            "          FeatureExtractor/resnet_v1_50/fpn/projection_2/biases (256, 256/256 params)\n",
            "          FeatureExtractor/resnet_v1_50/fpn/projection_2/weights (1x1x1024x256, 262.14k/262.14k params)\n",
            "        FeatureExtractor/resnet_v1_50/fpn/projection_3 (--/524.54k params)\n",
            "          FeatureExtractor/resnet_v1_50/fpn/projection_3/biases (256, 256/256 params)\n",
            "          FeatureExtractor/resnet_v1_50/fpn/projection_3/weights (1x1x2048x256, 524.29k/524.29k params)\n",
            "        FeatureExtractor/resnet_v1_50/fpn/smoothing_1 (--/589.82k params)\n",
            "          FeatureExtractor/resnet_v1_50/fpn/smoothing_1/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/resnet_v1_50/fpn/smoothing_1/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "        FeatureExtractor/resnet_v1_50/fpn/smoothing_2 (--/589.82k params)\n",
            "          FeatureExtractor/resnet_v1_50/fpn/smoothing_2/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/resnet_v1_50/fpn/smoothing_2/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "  WeightSharedConvolutionalBoxPredictor (--/4.82m params)\n",
            "    WeightSharedConvolutionalBoxPredictor/BoxPredictionTower (--/2.36m params)\n",
            "      WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0 (--/589.82k params)\n",
            "        WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/BatchNorm (--/0 params)\n",
            "        WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_0/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "      WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1 (--/589.82k params)\n",
            "        WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/BatchNorm (--/0 params)\n",
            "        WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_1/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "      WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2 (--/589.82k params)\n",
            "        WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/BatchNorm (--/0 params)\n",
            "        WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_2/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "      WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3 (--/589.82k params)\n",
            "        WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/BatchNorm (--/0 params)\n",
            "        WeightSharedConvolutionalBoxPredictor/BoxPredictionTower/conv2d_3/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "    WeightSharedConvolutionalBoxPredictor/BoxPredictor (--/55.32k params)\n",
            "      WeightSharedConvolutionalBoxPredictor/BoxPredictor/biases (24, 24/24 params)\n",
            "      WeightSharedConvolutionalBoxPredictor/BoxPredictor/weights (3x3x256x24, 55.30k/55.30k params)\n",
            "    WeightSharedConvolutionalBoxPredictor/ClassPredictionTower (--/2.36m params)\n",
            "      WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0 (--/589.82k params)\n",
            "        WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/BatchNorm (--/0 params)\n",
            "        WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_0/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "      WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1 (--/589.82k params)\n",
            "        WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/BatchNorm (--/0 params)\n",
            "        WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_1/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "      WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2 (--/589.82k params)\n",
            "        WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/BatchNorm (--/0 params)\n",
            "        WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_2/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "      WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3 (--/589.82k params)\n",
            "        WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/BatchNorm (--/0 params)\n",
            "        WeightSharedConvolutionalBoxPredictor/ClassPredictionTower/conv2d_3/weights (3x3x256x256, 589.82k/589.82k params)\n",
            "    WeightSharedConvolutionalBoxPredictor/ClassPredictor (--/41.49k params)\n",
            "      WeightSharedConvolutionalBoxPredictor/ClassPredictor/biases (18, 18/18 params)\n",
            "      WeightSharedConvolutionalBoxPredictor/ClassPredictor/weights (3x3x256x18, 41.47k/41.47k params)\n",
            "\n",
            "======================End of Report==========================\n",
            "177 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              1\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   float_ops\n",
            "-account_type_regexes       .*\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     float_ops\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
            "\n",
            "Profile:\n",
            "node name | # float_ops\n",
            "_TFProfRoot (--/512.00k flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator/mul_5 (76.80k/76.80k flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator/mul_6 (76.80k/76.80k flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator/sub (76.80k/76.80k flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates/Scale/mul (38.40k/38.40k flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates/Scale/mul_1 (38.40k/38.40k flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates/Scale/mul_3 (38.40k/38.40k flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates/Scale/mul_2 (38.40k/38.40k flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_1/sub (19.20k/19.20k flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_1/mul_6 (19.20k/19.20k flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_1/mul_5 (19.20k/19.20k flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_1/Scale/mul_1 (9.60k/9.60k flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_1/Scale/mul_2 (9.60k/9.60k flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_1/Scale/mul (9.60k/9.60k flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_1/Scale/mul_3 (9.60k/9.60k flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_2/sub (4.80k/4.80k flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_2/mul_6 (4.80k/4.80k flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_2/mul_5 (4.80k/4.80k flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_2/Scale/mul_3 (2.40k/2.40k flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_2/Scale/mul_2 (2.40k/2.40k flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_2/Scale/mul_1 (2.40k/2.40k flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_2/Scale/mul (2.40k/2.40k flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_3/mul_5 (1.20k/1.20k flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_3/mul_6 (1.20k/1.20k flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_3/sub (1.20k/1.20k flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_3/Scale/mul (600/600 flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_3/Scale/mul_3 (600/600 flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_3/Scale/mul_2 (600/600 flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_3/Scale/mul_1 (600/600 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_4/sub (300/300 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_4/mul_6 (300/300 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_4/mul_5 (300/300 flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_4/Scale/mul (150/150 flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_4/Scale/mul_1 (150/150 flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_4/Scale/mul_2 (150/150 flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_4/Scale/mul_3 (150/150 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator/mul_4 (80/80 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator/mul_3 (80/80 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_1/mul_4 (40/40 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_1/mul_3 (40/40 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_2/mul_4 (20/20 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_2/mul_3 (20/20 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_3/mul_3 (10/10 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_3/mul_4 (10/10 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_4/truediv (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator/mul (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator/truediv (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_2/mul (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_2/mul_1 (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_1/truediv (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_1/mul_2 (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_2/mul_2 (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_2/truediv (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_1/mul_1 (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_3/mul (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_1/mul (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_3/mul_1 (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_3/mul_2 (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_3/truediv (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator/mul_2 (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_4/mul (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_4/mul_1 (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_4/mul_2 (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator/mul_1 (6/6 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_4/mul_3 (5/5 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_4/mul_4 (5/5 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n",
            "  Preprocessor/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_18 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_19 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n",
            "  Preprocessor/map/while/Less (1/1 flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_2/truediv (1/1 flops)\n",
            "  MultiscaleGridAnchorGenerator/assert_equal_1/Equal (1/1 flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_4/truediv_1 (1/1 flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_4/truediv (1/1 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator/assert_equal_1/Equal (1/1 flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_3/truediv_1 (1/1 flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_3/truediv (1/1 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_1/assert_equal_1/Equal (1/1 flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_2/truediv_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_2/assert_equal_1/Equal (1/1 flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_1/truediv_1 (1/1 flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates_1/truediv (1/1 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_3/assert_equal_1/Equal (1/1 flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates/truediv_1 (1/1 flops)\n",
            "  MultiscaleGridAnchorGenerator/ToNormalizedCoordinates/truediv (1/1 flops)\n",
            "  MultiscaleGridAnchorGenerator/GridAnchorGenerator_4/assert_equal_1/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_3 (1/1 flops)\n",
            "\n",
            "======================End of Report==========================\n",
            "2020-07-07 10:41:44.346813: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-07-07 10:41:44.364773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:44.365310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-07-07 10:41:44.365612: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-07-07 10:41:44.371085: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-07-07 10:41:44.372704: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-07-07 10:41:44.373034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-07-07 10:41:44.387240: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-07-07 10:41:44.400316: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-07-07 10:41:44.411566: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-07-07 10:41:44.411734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:44.412312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:44.412848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-07-07 10:41:44.413258: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
            "2020-07-07 10:41:44.428322: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000179999 Hz\n",
            "2020-07-07 10:41:44.428505: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28dd640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-07-07 10:41:44.428532: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-07-07 10:41:44.516800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:44.517461: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28dcd80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-07-07 10:41:44.517493: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-07-07 10:41:44.517709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:44.518233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-07-07 10:41:44.518306: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-07-07 10:41:44.518333: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-07-07 10:41:44.518361: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-07-07 10:41:44.518381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-07-07 10:41:44.518400: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-07-07 10:41:44.518422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-07-07 10:41:44.518444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-07-07 10:41:44.518519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:44.519091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:44.519571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-07-07 10:41:44.519647: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-07-07 10:41:44.520795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-07-07 10:41:44.520821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2020-07-07 10:41:44.520835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2020-07-07 10:41:44.520941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:44.521478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:44.522003: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-07-07 10:41:44.522045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-600\n",
            "I0707 10:41:44.523698 139766366193536 saver.py:1284] Restoring parameters from training/model.ckpt-600\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W0707 10:41:46.135504 139766366193536 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2020-07-07 10:41:46.882303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:46.882887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-07-07 10:41:46.882974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-07-07 10:41:46.883015: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-07-07 10:41:46.883039: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-07-07 10:41:46.883063: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-07-07 10:41:46.883085: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-07-07 10:41:46.883106: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-07-07 10:41:46.883126: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-07-07 10:41:46.883209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:46.883759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:46.884252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-07-07 10:41:46.884291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-07-07 10:41:46.884304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2020-07-07 10:41:46.884314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2020-07-07 10:41:46.884402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:46.884939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:46.885433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-600\n",
            "I0707 10:41:46.886583 139766366193536 saver.py:1284] Restoring parameters from training/model.ckpt-600\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "W0707 10:41:48.212760 139766366193536 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "W0707 10:41:48.213030 139766366193536 deprecation.py:323] From /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "INFO:tensorflow:Froze 463 variables.\n",
            "I0707 10:41:48.699539 139766366193536 graph_util_impl.py:334] Froze 463 variables.\n",
            "INFO:tensorflow:Converted 463 variables to const ops.\n",
            "I0707 10:41:48.890919 139766366193536 graph_util_impl.py:394] Converted 463 variables to const ops.\n",
            "2020-07-07 10:41:49.274623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:49.275567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-07-07 10:41:49.275713: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-07-07 10:41:49.275748: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2020-07-07 10:41:49.275770: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2020-07-07 10:41:49.275796: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2020-07-07 10:41:49.275819: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-07-07 10:41:49.275840: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-07-07 10:41:49.275861: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-07-07 10:41:49.275967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:49.276888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:49.277691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2020-07-07 10:41:49.277744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-07-07 10:41:49.277760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2020-07-07 10:41:49.277773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2020-07-07 10:41:49.277893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:49.278845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-07-07 10:41:49.279701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:384: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W0707 10:41:49.918100 139766366193536 deprecation.py:323] From /content/models/research/object_detection/exporter.py:384: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:No assets to save.\n",
            "I0707 10:41:49.918815 139766366193536 builder_impl.py:640] No assets to save.\n",
            "INFO:tensorflow:No assets to write.\n",
            "I0707 10:41:49.918959 139766366193536 builder_impl.py:460] No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: ./fine_tuned_model/saved_model/saved_model.pb\n",
            "I0707 10:41:50.497561 139766366193536 builder_impl.py:425] SavedModel written to: ./fine_tuned_model/saved_model/saved_model.pb\n",
            "INFO:tensorflow:Writing pipeline config file to ./fine_tuned_model/pipeline.config\n",
            "I0707 10:41:50.568529 139766366193536 config_util.py:254] Writing pipeline config file to ./fine_tuned_model/pipeline.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usgBZvkz0nqD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "65d0ac7f-ff2e-4e06-a3df-9fff5248edbf"
      },
      "source": [
        "!ls {output_directory}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\t\t\tmodel.ckpt.index  saved_model\n",
            "frozen_inference_graph.pb\tmodel.ckpt.meta\n",
            "model.ckpt.data-00000-of-00001\tpipeline.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p09AOThWkaQv",
        "colab_type": "text"
      },
      "source": [
        "## Dowload do modelo, arquivo `.pb`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnDo1lonKgFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\")\n",
        "assert os.path.isfile(pb_fname), '`{}` not exist'.format(pb_fname)\n",
        "\n",
        "checkpoint_meta = os.path.join(os.path.abspath(output_directory), \"model.ckpt.meta\")\n",
        "checkpoint_index = os.path.join(os.path.abspath(output_directory), \"model.ckpt.index\")\n",
        "checkpoint_data = os.path.join(os.path.abspath(output_directory), \"model.ckpt.data-00000-of-00001\")\n",
        "checkpoint = os.path.join(os.path.abspath(output_directory), \"checkpoint\")\n",
        "saved_model = os.path.join(os.path.abspath(output_directory), \"saved_model/saved_model.pb\")\n",
        "\n",
        "#tf_log = os.path.join(os.path.abspath('/content/models/research/training/eval_0'), \"events.out.tfevents.1576020673.0083b462c1a8\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FKFq8RXs6bs",
        "colab_type": "text"
      },
      "source": [
        "### O dowload é feito diretamente na sua máquina pessoal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bP0iMMnnr77",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d02bb14a-1420-4598-edde-355e355a54d3"
      },
      "source": [
        "from google.colab import files\n",
        "# files.download(pb_fname)\n",
        "# files.download(label_map_pbtxt_fname)\n",
        "# files.download(pipeline_fname)\n",
        "# files.download(checkpoint_meta)\n",
        "# files.download(checkpoint_index)\n",
        "# files.download(checkpoint_data)\n",
        "files.download(saved_model)\n",
        "# files.download(checkpoint)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_69bf2129-80a6-4077-8af8-dbca33b162fc\", \"saved_model.pb\", 127754266)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz1gX19GlVW7",
        "colab_type": "text"
      },
      "source": [
        "## Rodar o teste de inferência do novo modelo!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG5YUMdg1Po7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/models/research/object_detection\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
        "PATH_TO_CKPT = pb_fname\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = label_map_pbtxt_fname\n",
        "\n",
        "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
        "PATH_TO_TEST_IMAGES_DIR =  os.path.join(repo_dir_path, \"test\")\n",
        "\n",
        "assert os.path.isfile(pb_fname)\n",
        "assert os.path.isfile(PATH_TO_LABELS)\n",
        "TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.*\"))\n",
        "assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n",
        "print(TEST_IMAGE_PATHS)\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "\n",
        "# This is needed to display the images.\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map, max_num_classes=num_classes, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "\n",
        "def load_image_into_numpy_array(image):\n",
        "    (im_width, im_height) = image.size\n",
        "    return np.array(image.getdata()).reshape(\n",
        "        (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (12, 8)\n",
        "\n",
        "\n",
        "def run_inference_for_single_image(image, graph):\n",
        "    with graph.as_default():\n",
        "        with tf.Session() as sess:\n",
        "            # Get handles to input and output tensors\n",
        "            ops = tf.get_default_graph().get_operations()\n",
        "            all_tensor_names = {\n",
        "                output.name for op in ops for output in op.outputs}\n",
        "            tensor_dict = {}\n",
        "            for key in [\n",
        "                'num_detections', 'detection_boxes', 'detection_scores',\n",
        "                'detection_classes', 'detection_masks'\n",
        "            ]:\n",
        "                tensor_name = key + ':0'\n",
        "                if tensor_name in all_tensor_names:\n",
        "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "                        tensor_name)\n",
        "            if 'detection_masks' in tensor_dict:\n",
        "                # The following processing is only for single image\n",
        "                detection_boxes = tf.squeeze(\n",
        "                    tensor_dict['detection_boxes'], [0])\n",
        "                detection_masks = tf.squeeze(\n",
        "                    tensor_dict['detection_masks'], [0])\n",
        "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "                real_num_detection = tf.cast(\n",
        "                    tensor_dict['num_detections'][0], tf.int32)\n",
        "                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n",
        "                                           real_num_detection, -1])\n",
        "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n",
        "                                           real_num_detection, -1, -1])\n",
        "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "                detection_masks_reframed = tf.cast(\n",
        "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "                # Follow the convention by adding back the batch dimension\n",
        "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "                    detection_masks_reframed, 0)\n",
        "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "            # Run inference\n",
        "            output_dict = sess.run(tensor_dict,\n",
        "                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "            output_dict['num_detections'] = int(\n",
        "                output_dict['num_detections'][0])\n",
        "            output_dict['detection_classes'] = output_dict[\n",
        "                'detection_classes'][0].astype(np.uint8)\n",
        "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "            if 'detection_masks' in output_dict:\n",
        "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "    return output_dict\n",
        "\n",
        "\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "    image = Image.open(image_path)\n",
        "    # the array based representation of the image will be used later in order to prepare the\n",
        "    # result image with boxes and labels on it.\n",
        "    image_np = load_image_into_numpy_array(image)\n",
        "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "    # Actual detection.\n",
        "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "    # Visualization of the results of a detection.\n",
        "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np,\n",
        "        output_dict['detection_boxes'],\n",
        "        output_dict['detection_classes'],\n",
        "        output_dict['detection_scores'],\n",
        "        category_index,\n",
        "        instance_masks=output_dict.get('detection_masks'),\n",
        "        use_normalized_coordinates=True,\n",
        "        line_thickness=8)\n",
        "    plt.figure(figsize=IMAGE_SIZE)\n",
        "    plt.imshow(image_np)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}